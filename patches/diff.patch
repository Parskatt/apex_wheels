diff --git a/setup.py b/setup.py
index fa61b72..98042ba 100644
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,4 @@
+from functools import partial
 import sys
 import warnings
 import os
@@ -186,6 +187,15 @@ if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 4):
 version_dependent_macros = version_ge_1_1 + version_ge_1_3 + version_ge_1_5
 
 _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)
+cuda_suffix = f"cu{bare_metal_version.major}-{bare_metal_version.minor}"
+torch_suffix = f"torch{TORCH_MAJOR}-{TORCH_MINOR}"
+
+CUDAExtension = partial(
+    CUDAExtension,
+    runtime_library_dirs=[
+        "$ORIGIN/torch/lib",
+        ]
+    )
 
 if has_flag("--distributed_adam", "APEX_DISTRIBUTED_ADAM"):
     if "--distributed_adam" in sys.argv:
@@ -532,11 +542,15 @@ if has_flag("--group_norm", "APEX_GROUP_NORM"):
 
     # CUDA group norm supports from SM70
     arch_flags = []
-    # FIXME: this needs to be done more cleanly
-    for arch in [70, 75, 80, 86, 90, 100, 120]:
-        arch_flag = f"-gencode=arch=compute_{arch},code=sm_{arch}"
-        arch_flags.append(arch_flag)
-    arch_flags.append(arch_flag)
+    arch_flags.append("-gencode=arch=compute_70,code=sm_70")
+    if bare_metal_version >= Version("11.0"):
+        arch_flags.append("-gencode=arch=compute_75,code=sm_75")
+        arch_flags.append("-gencode=arch=compute_80,code=sm_80")
+    if bare_metal_version >= Version("11.8"):
+        arch_flags.append("-gencode=arch=compute_90,code=sm_90")
+    if bare_metal_version >= Version("12.8"):
+        arch_flags.append("-gencode=arch=compute_100,code=sm_100")
+        arch_flags.append("-gencode=arch=compute_120,code=sm_120")
 
     ext_modules.append(
         CUDAExtension(
@@ -555,31 +569,35 @@ if has_flag("--group_norm", "APEX_GROUP_NORM"):
     )
 
     # CUDA group norm V2 is tested on SM100
-    if bare_metal_version >= Version("12.8"):
-        arch_flags = ["-gencode=arch=compute_100,code=sm_100"]
-    else:
-        arch_flags = ["-gencode=arch=compute_90,code=compute_90"]
-
-    ext_modules.append(
-        CUDAExtension(
-            name="group_norm_v2_cuda",
-            sources=[
-                "apex/contrib/csrc/group_norm_v2/gn.cpp",
-                "apex/contrib/csrc/group_norm_v2/gn_cuda.cu",
-                "apex/contrib/csrc/group_norm_v2/gn_utils.cpp",
-            ] + glob.glob("apex/contrib/csrc/group_norm_v2/gn_cuda_inst_*.cu"),
-            extra_compile_args={
-                "cxx": ["-O2"] + version_dependent_macros,
-                "nvcc": [
-                    "-O2", "--use_fast_math", "--ftz=false",
-                    "-U__CUDA_NO_HALF_CONVERSIONS__",
-                    "-U__CUDA_NO_HALF_OPERATORS__",
-                    "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
-                    "-U__CUDA_NO_BFLOAT16_OPERATORS__",
-                ] + arch_flags + version_dependent_macros,
-            },
+    if bare_metal_version >= Version("12.4"):
+        if bare_metal_version >= Version("12.8"):
+            arch_flags = [
+                "-gencode=arch=compute_90,code=sm_90",
+                "-gencode=arch=compute_100,code=sm_100",
+                "-gencode=arch=compute_120,code=compute_120",
+            ]
+        else:
+            arch_flags = ["-gencode=arch=compute_90,code=compute_90"]
+        ext_modules.append(
+            CUDAExtension(
+                name="group_norm_v2_cuda",
+                sources=[
+                    "apex/contrib/csrc/group_norm_v2/gn.cpp",
+                    "apex/contrib/csrc/group_norm_v2/gn_cuda.cu",
+                    "apex/contrib/csrc/group_norm_v2/gn_utils.cpp",
+                ] + glob.glob("apex/contrib/csrc/group_norm_v2/gn_cuda_inst_*.cu"),
+                extra_compile_args={
+                    "cxx": ["-O2"] + version_dependent_macros,
+                    "nvcc": [
+                        "-O2", "--use_fast_math", "--ftz=false",
+                        "-U__CUDA_NO_HALF_CONVERSIONS__",
+                        "-U__CUDA_NO_HALF_OPERATORS__",
+                        "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
+                        "-U__CUDA_NO_BFLOAT16_OPERATORS__",
+                    ] + arch_flags + version_dependent_macros,
+                },
+            )
         )
-    )
 
 if has_flag("--index_mul_2d", "APEX_INDEX_MUL_2D"):
     if "--index_mul_2d" in sys.argv:
@@ -1027,7 +1045,7 @@ class BuildExtensionSeparateDir(BuildExtension):
 
 
 setup(
-    name="apex",
+    name=f"apex-johan-{torch_suffix}-{cuda_suffix}",
     version="0.1",
     packages=find_packages(
         exclude=("build", "csrc", "include", "tests", "dist", "docs", "tests", "examples", "apex.egg-info",)
